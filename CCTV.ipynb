{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "47a12336",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc770e85384a46f8b781a19f7067500d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/3.99G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded: A:\\hf_models\\Qwen2-VL-2B-Instruct\\model-00001-of-00002.safetensors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8cf36a87902140829d411f951a747032",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/429M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded: A:\\hf_models\\Qwen2-VL-2B-Instruct\\model-00002-of-00002.safetensors\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import hf_hub_download\n",
    "\n",
    "repo_id = \"Qwen/Qwen2-VL-2B-Instruct\"\n",
    "local_dir = r\"A:\\hf_models\\Qwen2-VL-2B-Instruct\"\n",
    "\n",
    "for fname in [\n",
    "    \"model-00001-of-00002.safetensors\",\n",
    "    \"model-00002-of-00002.safetensors\",\n",
    "]:\n",
    "    path = hf_hub_download(\n",
    "        repo_id=repo_id,\n",
    "        filename=fname,\n",
    "        local_dir=local_dir,\n",
    "        force_download=True,\n",
    "    )\n",
    "    print(\"Downloaded:\", path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0cf4565d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The image processor of type `Qwen2VLImageProcessor` is now loaded as a fast processor by default, even if the model checkpoint was saved with a slow processor. This is a breaking change and may produce slightly different outputs. To continue using the slow processor, instantiate this class with `use_fast=False`. \n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoProcessor\n",
    "processor = AutoProcessor.from_pretrained(\n",
    "    r\"A:\\hf_models\\Qwen2-VL-2B-Instruct\",\n",
    "    min_pixels=256*28*28,\n",
    "    max_pixels=1024*28*28,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cec2ed26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88aa247333c24ce188739e4e6957ea3b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/729 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model device: cuda:0\n",
      "Speed optimizations: 4 frames/clip, 80 max tokens, resolution 128-512 visual tokens\n",
      "FPS: 60.00287306574532, Total frames: 181, Clip size: 120 frames (2.0s)\n",
      "Estimated 1 clips to process\n",
      "\n",
      "Clip 000 | 00:00:00 - 00:00:01 | SKIP  | (no significant activity)\n",
      "Clip 001 | 00:00:01 - 00:00:03 | LOG   | 30.6s | ALERT: Accident\n",
      "\n",
      "Done in 0.6 minutes.\n",
      "Processed 2 clips (1 skipped for low motion)\n",
      "Average: 34.8s per analyzed clip\n",
      "Readable log: outputs\\cctv_clips.txt\n",
      "JSONL log: outputs\\cctv_clips.jsonl\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import time\n",
    "from collections import deque\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "from transformers import Qwen2VLForConditionalGeneration, AutoProcessor\n",
    "from qwen_vl_utils import process_vision_info\n",
    "\n",
    "\n",
    "# =========================\n",
    "# USER CONFIG\n",
    "# =========================\n",
    "VIDEO_PATH = r\"A:\\Context Aware CCTV Surviellance system\\accident.mp4\"\n",
    "MODEL_ID = r\"A:\\hf_models\\Qwen2-VL-2B-Instruct\"\n",
    "\n",
    "# SPEED OPTIMIZATIONS:\n",
    "CLIP_DURATION_SEC = 2.0\n",
    "FRAMES_PER_CLIP = 4              # Reduced from 5 → 40% faster preprocessing\n",
    "MOTION_THRESH_CLIP = 0.5         # Skip boring clips (lower = more sensitive)\n",
    "\n",
    "# Image resolution (lower = faster, but less detail)\n",
    "MIN_PIXELS = 128 * 28 * 28       # Reduced from 256 → faster image encoding\n",
    "MAX_PIXELS = 512 * 28 * 28       # Reduced from 1024 → faster image encoding\n",
    "\n",
    "# Generation\n",
    "MAX_NEW_TOKENS = 80              # Reduced from 150 → faster generation\n",
    "\n",
    "# Alert smoothing\n",
    "WINDOW_N = 3\n",
    "TRIGGER_K = 2\n",
    "ALERT_COOLDOWN_SEC = 10\n",
    "\n",
    "# Output\n",
    "OUT_DIR = \"outputs\"\n",
    "LOG_TXT = os.path.join(OUT_DIR, \"cctv_clips.txt\")\n",
    "LOG_JSONL = os.path.join(OUT_DIR, \"cctv_clips.jsonl\")\n",
    "\n",
    "\n",
    "# =========================\n",
    "# HELPERS\n",
    "# =========================\n",
    "def ensure_dir(p):\n",
    "    os.makedirs(p, exist_ok=True)\n",
    "\n",
    "def fmt_hhmmss(seconds: float) -> str:\n",
    "    seconds = int(max(0, seconds))\n",
    "    h = seconds // 3600\n",
    "    m = (seconds % 3600) // 60\n",
    "    s = seconds % 60\n",
    "    return f\"{h:02d}:{m:02d}:{s:02d}\"\n",
    "\n",
    "def fmt_time_range(start_sec: float, end_sec: float) -> str:\n",
    "    return f\"{fmt_hhmmss(start_sec)} - {fmt_hhmmss(end_sec)}\"\n",
    "\n",
    "def write_line(path, line: str):\n",
    "    with open(path, \"a\", encoding=\"utf-8\") as f:\n",
    "        f.write(line.rstrip() + \"\\n\")\n",
    "\n",
    "def write_jsonl(path, obj: dict):\n",
    "    import json\n",
    "    with open(path, \"a\", encoding=\"utf-8\") as f:\n",
    "        f.write(json.dumps(obj, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "def motion_score(prev_bgr, curr_bgr) -> float:\n",
    "    prev = cv2.cvtColor(prev_bgr, cv2.COLOR_BGR2GRAY)\n",
    "    curr = cv2.cvtColor(curr_bgr, cv2.COLOR_BGR2GRAY)\n",
    "    prev = cv2.resize(prev, (160, 90))\n",
    "    curr = cv2.resize(curr, (160, 90))\n",
    "    return float(np.mean(cv2.absdiff(prev, curr)))\n",
    "\n",
    "\n",
    "# =========================\n",
    "# LOAD MODEL\n",
    "# =========================\n",
    "ensure_dir(OUT_DIR)\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    min_pixels=MIN_PIXELS,\n",
    "    max_pixels=MAX_PIXELS,\n",
    ")\n",
    "\n",
    "model = Qwen2VLForConditionalGeneration.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    torch_dtype=torch.float16,    # Use FP16 for faster inference\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "model.eval()\n",
    "\n",
    "print(\"Model device:\", next(model.parameters()).device)\n",
    "print(f\"Speed optimizations: {FRAMES_PER_CLIP} frames/clip, {MAX_NEW_TOKENS} max tokens, resolution {MIN_PIXELS//784}-{MAX_PIXELS//784} visual tokens\")\n",
    "\n",
    "\n",
    "def vlm_describe_clip(frames_pil: list, time_range: str):\n",
    "    \"\"\"\n",
    "    frames_pil: list of PIL images sampled from the clip\n",
    "    Returns: (model_tag, detailed_description, raw_line)\n",
    "    \"\"\"\n",
    "    # Shorter prompt = faster processing\n",
    "    content = [\n",
    "        {\"type\": \"text\", \"text\": f\"CCTV {time_range}. {len(frames_pil)} frames in order.\"}\n",
    "    ]\n",
    "    for pil_img in frames_pil:\n",
    "        content.append({\"type\": \"image\", \"image\": pil_img})\n",
    "    \n",
    "    content.append({\"type\": \"text\", \"text\":\n",
    "        \"In maximum 2 sentences: what's happening? \"\n",
    "        \"If suspicious (lethals, robbery, assault, weapon, accident, fire, vandalism, forced entry, running), start with ALERT:. \"\n",
    "        \"Otherwise start with LOG:\"\n",
    "    })\n",
    "\n",
    "    messages = [{\"role\": \"user\", \"content\": content}]\n",
    "\n",
    "    prompt = processor.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True,\n",
    "        add_vision_id=True,\n",
    "    )\n",
    "\n",
    "    images, videos = process_vision_info(messages)\n",
    "    inputs = processor(\n",
    "        text=[prompt],\n",
    "        images=images,\n",
    "        videos=videos,\n",
    "        padding=True,\n",
    "        return_tensors=\"pt\",\n",
    "    ).to(model.device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Mixed precision inference for speed\n",
    "        with torch.amp.autocast('cuda', dtype=torch.float16):\n",
    "            output_ids = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=MAX_NEW_TOKENS,\n",
    "                do_sample=False,\n",
    "                num_beams=1,\n",
    "                use_cache=True,\n",
    "            )\n",
    "\n",
    "\n",
    "    # Decode only new tokens (trim prompt)\n",
    "    trimmed = [out_ids[len(in_ids):] for in_ids, out_ids in zip(inputs.input_ids, output_ids)]\n",
    "    text = processor.batch_decode(\n",
    "        trimmed,\n",
    "        skip_special_tokens=True,\n",
    "        clean_up_tokenization_spaces=True,\n",
    "    )[0].strip()\n",
    "\n",
    "    # Parse tag\n",
    "    up = text.upper()\n",
    "    if up.startswith(\"ALERT:\"):\n",
    "        tag = \"ALERT\"\n",
    "        desc = text.split(\":\", 1)[1].strip() if \":\" in text else text\n",
    "    elif up.startswith(\"LOG:\"):\n",
    "        tag = \"LOG\"\n",
    "        desc = text.split(\":\", 1)[1].strip() if \":\" in text else text\n",
    "    else:\n",
    "        tag = \"LOG\"\n",
    "        desc = text\n",
    "\n",
    "    raw_line = f\"{tag}: {desc}\"\n",
    "    return tag, desc, raw_line\n",
    "\n",
    "\n",
    "# =========================\n",
    "# VIDEO LOOP (CLIP-BASED)\n",
    "# =========================\n",
    "cap = cv2.VideoCapture(VIDEO_PATH)\n",
    "if not cap.isOpened():\n",
    "    raise RuntimeError(f\"Could not open video: {VIDEO_PATH}\")\n",
    "\n",
    "fps = cap.get(cv2.CAP_PROP_FPS) or 25.0\n",
    "total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "clip_frames_count = int(fps * CLIP_DURATION_SEC)\n",
    "\n",
    "print(f\"FPS: {fps}, Total frames: {total_frames}, Clip size: {clip_frames_count} frames ({CLIP_DURATION_SEC}s)\")\n",
    "print(f\"Estimated {total_frames // clip_frames_count} clips to process\\n\")\n",
    "\n",
    "# Alert smoothing\n",
    "decision_window = deque(maxlen=WINDOW_N)\n",
    "last_alert_time = 0\n",
    "\n",
    "clip_num = 0\n",
    "skipped_low_motion = 0\n",
    "start_time = time.time()\n",
    "\n",
    "while True:\n",
    "    # Calculate clip boundaries\n",
    "    clip_start_frame = clip_num * clip_frames_count\n",
    "    clip_end_frame = clip_start_frame + clip_frames_count\n",
    "    \n",
    "    if clip_start_frame >= total_frames:\n",
    "        break\n",
    "    \n",
    "    # Set video position to clip start\n",
    "    cap.set(cv2.CAP_PROP_POS_FRAMES, clip_start_frame)\n",
    "    \n",
    "    clip_start_sec = clip_start_frame / fps\n",
    "    clip_end_sec = min(clip_end_frame, total_frames) / fps\n",
    "    time_range = fmt_time_range(clip_start_sec, clip_end_sec)\n",
    "    \n",
    "    # Read all frames in this clip\n",
    "    clip_frames_bgr = []\n",
    "    motion_scores = []\n",
    "    prev_bgr = None\n",
    "    \n",
    "    for local_idx in range(clip_frames_count):\n",
    "        ok, frame_bgr = cap.read()\n",
    "        if not ok:\n",
    "            break\n",
    "        clip_frames_bgr.append(frame_bgr)\n",
    "        \n",
    "        # Track motion\n",
    "        if prev_bgr is not None:\n",
    "            motion_scores.append(motion_score(prev_bgr, frame_bgr))\n",
    "        prev_bgr = frame_bgr\n",
    "    \n",
    "    if len(clip_frames_bgr) < FRAMES_PER_CLIP:\n",
    "        clip_num += 1\n",
    "        continue\n",
    "    \n",
    "    avg_motion = np.mean(motion_scores) if motion_scores else 0.0\n",
    "    \n",
    "    # SPEED GATE: skip low-motion clips (static scene)\n",
    "    if avg_motion < MOTION_THRESH_CLIP:\n",
    "        line = f\"Clip {clip_num:03d} | {time_range} | SKIP  | (no significant activity)\"\n",
    "        print(line)\n",
    "        write_line(LOG_TXT, line)\n",
    "        write_jsonl(LOG_JSONL, {\n",
    "            \"clip_num\": clip_num,\n",
    "            \"time_range\": time_range,\n",
    "            \"start_sec\": round(clip_start_sec, 3),\n",
    "            \"end_sec\": round(clip_end_sec, 3),\n",
    "            \"system_tag\": \"SKIP\",\n",
    "            \"avg_motion\": round(avg_motion, 3),\n",
    "            \"note\": \"low_motion_skip\",\n",
    "        })\n",
    "        skipped_low_motion += 1\n",
    "        clip_num += 1\n",
    "        continue\n",
    "    \n",
    "    # Sample representative frames evenly from the clip\n",
    "    indices = np.linspace(0, len(clip_frames_bgr) - 1, FRAMES_PER_CLIP, dtype=int)\n",
    "    sampled_bgr = [clip_frames_bgr[i] for i in indices]\n",
    "    \n",
    "    # Convert to PIL\n",
    "    frames_pil = []\n",
    "    for bgr in sampled_bgr:\n",
    "        rgb = cv2.cvtColor(bgr, cv2.COLOR_BGR2RGB)\n",
    "        frames_pil.append(Image.fromarray(rgb))\n",
    "    \n",
    "    # Get VLM description for this clip\n",
    "    clip_inference_start = time.time()\n",
    "    model_tag, desc, raw_line = vlm_describe_clip(frames_pil, time_range)\n",
    "    clip_inference_time = time.time() - clip_inference_start\n",
    "    \n",
    "    # Smooth alerts\n",
    "    positive = (model_tag == \"ALERT\")\n",
    "    decision_window.append(1 if positive else 0)\n",
    "    \n",
    "    should_alert = (sum(decision_window) >= TRIGGER_K) and ((time.time() - last_alert_time) > ALERT_COOLDOWN_SEC)\n",
    "    system_tag = \"ALERT\" if should_alert else \"LOG\"\n",
    "    if should_alert:\n",
    "        last_alert_time = time.time()\n",
    "    \n",
    "    # Log output with timing\n",
    "    line = f\"Clip {clip_num:03d} | {time_range} | {system_tag:<5} | {clip_inference_time:.1f}s | {raw_line}\"\n",
    "    print(line)\n",
    "    write_line(LOG_TXT, line)\n",
    "    \n",
    "    write_jsonl(LOG_JSONL, {\n",
    "        \"clip_num\": clip_num,\n",
    "        \"time_range\": time_range,\n",
    "        \"start_sec\": round(clip_start_sec, 3),\n",
    "        \"end_sec\": round(clip_end_sec, 3),\n",
    "        \"system_tag\": system_tag,\n",
    "        \"model_tag\": model_tag,\n",
    "        \"avg_motion\": round(avg_motion, 3),\n",
    "        \"inference_time_sec\": round(clip_inference_time, 3),\n",
    "        \"description\": desc,\n",
    "        \"raw_line\": raw_line,\n",
    "        \"window_sum\": int(sum(decision_window)),\n",
    "    })\n",
    "    \n",
    "    clip_num += 1\n",
    "\n",
    "cap.release()\n",
    "elapsed = time.time() - start_time\n",
    "print(f\"\\nDone in {elapsed/60:.1f} minutes.\")\n",
    "print(f\"Processed {clip_num} clips ({skipped_low_motion} skipped for low motion)\")\n",
    "print(f\"Average: {elapsed/max(1, clip_num-skipped_low_motion):.1f}s per analyzed clip\")\n",
    "print(\"Readable log:\", LOG_TXT)\n",
    "print(\"JSONL log:\", LOG_JSONL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6a98dd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The image processor of type `Qwen2VLImageProcessor` is now loaded as a fast processor by default, even if the model checkpoint was saved with a slow processor. This is a breaking change and may produce slightly different outputs. To continue using the slow processor, instantiate this class with `use_fast=False`. \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c28b0b479fb54d3c8b20b91bd5d4a074",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/729 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model device: cuda:0\n",
      "Config: 3 frames/clip, 80 tokens, resolution 256-1024 visual tokens\n",
      "\n",
      "Original FPS: 8.0, Target FPS: 8.0, Frame skip: 1\n",
      "Total frames: 701, Clip size: 16 frames (2.0s)\n",
      "Estimated 43 clips to process\n",
      "\n",
      "Clip 000 | 00:00:00 - 00:00:02 | SKIP  | motion=0.15 | (no significant activity)\n",
      "Clip 001 | 00:00:02 - 00:00:04 | SKIP  | motion=0.24 | (no significant activity)\n",
      "Clip 002 | 00:00:04 - 00:00:06 | SKIP  | motion=0.19 | (no significant activity)\n",
      "Clip 003 | 00:00:06 - 00:00:08 | SKIP  | motion=0.05 | (no significant activity)\n",
      "Clip 004 | 00:00:08 - 00:00:10 | SKIP  | motion=0.09 | (no significant activity)\n",
      "Clip 005 | 00:00:10 - 00:00:12 | SKIP  | motion=0.04 | (no significant activity)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clip 006 | 00:00:12 - 00:00:14 | SKIP  | motion=0.02 | (no significant activity)\n",
      "Clip 007 | 00:00:14 - 00:00:16 | SKIP  | motion=0.09 | (no significant activity)\n",
      "Clip 008 | 00:00:16 - 00:00:18 | SKIP  | motion=0.36 | (no significant activity)\n",
      "Clip 009 | 00:00:18 - 00:00:20 | LOG   | motion=1.71 | 2.1s | ALERT: No suspicious activity detected.\n",
      "Clip 010 | 00:00:20 - 00:00:22 | ALERT | motion=3.90 | 1.1s | ALERT: No suspicious activity detected.\n",
      "Clip 011 | 00:00:22 - 00:00:24 | SKIP  | motion=0.27 | (no significant activity)\n",
      "Clip 012 | 00:00:24 - 00:00:26 | SKIP  | motion=0.71 | (no significant activity)\n",
      "Clip 013 | 00:00:26 - 00:00:28 | LOG   | motion=1.87 | 2.2s | ALERT: The scene appears to be a typical parking lot with various vehicles parked. There are no suspicious activities or incidents visible in the image.\n",
      "Clip 014 | 00:00:28 - 00:00:30 | LOG   | motion=1.81 | 1.1s | ALERT: No suspicious activity detected.\n",
      "Clip 015 | 00:00:30 - 00:00:32 | SKIP  | motion=0.05 | (no significant activity)\n",
      "Clip 016 | 00:00:32 - 00:00:34 | SKIP  | motion=0.08 | (no significant activity)\n",
      "Clip 017 | 00:00:34 - 00:00:36 | SKIP  | motion=0.09 | (no significant activity)\n",
      "Clip 018 | 00:00:36 - 00:00:38 | SKIP  | motion=0.12 | (no significant activity)\n",
      "Clip 019 | 00:00:38 - 00:00:40 | SKIP  | motion=0.33 | (no significant activity)\n",
      "Clip 020 | 00:00:40 - 00:00:42 | SKIP  | motion=0.18 | (no significant activity)\n",
      "Clip 021 | 00:00:42 - 00:00:44 | SKIP  | motion=0.08 | (no significant activity)\n",
      "Clip 022 | 00:00:44 - 00:00:46 | SKIP  | motion=0.15 | (no significant activity)\n",
      "Clip 023 | 00:00:46 - 00:00:48 | SKIP  | motion=0.21 | (no significant activity)\n",
      "Clip 024 | 00:00:48 - 00:00:50 | SKIP  | motion=0.45 | (no significant activity)\n",
      "Clip 025 | 00:00:50 - 00:00:52 | LOG   | motion=2.00 | 2.2s | ALERT: The scene appears to be a typical parking lot with several vehicles parked. There are no suspicious activities or incidents visible in the image.\n",
      "Clip 026 | 00:00:52 - 00:00:54 | LOG   | motion=1.44 | 2.2s | ALERT: The scene appears to be a typical parking lot with several vehicles parked. There are no suspicious activities or incidents visible in the video.\n",
      "Clip 027 | 00:00:54 - 00:00:56 | ALERT | motion=2.82 | 2.2s | ALERT: The scene appears to be a typical parking lot with several vehicles parked. There are no suspicious activities or incidents visible in the video.\n",
      "Clip 028 | 00:00:56 - 00:00:58 | LOG   | motion=2.18 | 4.1s | ALERT: The scene appears to be a parking lot with several vehicles, including a blue pickup truck and a black SUV. There are two individuals present, one standing near the blue pickup truck and the other walking towards the black SUV. The individuals seem to be engaged in a conversation or interaction.\n",
      "Clip 029 | 00:00:58 - 00:01:00 | LOG   | motion=1.71 | 2.5s | ALERT: The scene appears to be a typical parking lot with several vehicles parked. There are two individuals walking in the parking lot, and no suspicious activity is observed.\n",
      "Clip 030 | 00:01:00 - 00:01:02 | LOG   | motion=1.08 | 2.5s | ALERT: The scene appears to be a typical parking lot with several vehicles parked. There are two individuals walking in the parking lot, and no suspicious activity is observed.\n",
      "Clip 031 | 00:01:02 - 00:01:04 | ALERT | motion=1.09 | 2.3s | ALERT: The scene appears to be a typical parking lot with several vehicles parked. There are no suspicious activities or incidents visible in the video.\n",
      "Clip 032 | 00:01:04 - 00:01:06 | LOG   | motion=2.55 | 2.3s | ALERT: The scene appears to be a typical parking lot with several vehicles parked. There are no suspicious activities or incidents visible in the video.\n",
      "Clip 033 | 00:01:06 - 00:01:08 | LOG   | motion=2.89 | 2.2s | ALERT: The scene appears to be a typical parking lot with several vehicles parked. There are no suspicious activities or incidents visible in the video.\n",
      "Clip 034 | 00:01:08 - 00:01:10 | LOG   | motion=1.34 | 4.7s | ALERT: The scene appears to be a parking lot with several vehicles, including a blue pickup truck and a black car. There are three people standing near the edge of the lot, with one person holding a cell phone. The individuals are engaged in a conversation, and there is no indication of any suspicious activity or incident. The situation seems to be calm and normal.\n",
      "Clip 035 | 00:01:10 - 00:01:12 | ALERT | motion=1.12 | 4.6s | ALERT: The scene appears to be a parking lot with several vehicles, including a blue truck and a black car. There are three people standing near the edge of the parking lot, with one person holding a cell phone. The individuals seem to be engaged in a conversation or activity. There is no indication of any suspicious activity or incident in the image.\n",
      "Clip 036 | 00:01:12 - 00:01:14 | LOG   | motion=1.80 | 2.3s | ALERT: The scene appears to be a typical parking lot with several vehicles parked. There are no suspicious activities or incidents visible in the video.\n",
      "Clip 037 | 00:01:14 - 00:01:16 | LOG   | motion=2.36 | 4.9s | ALERT: The scene appears to be a parking lot with several vehicles, including a blue pickup truck and a black car. There are several people standing around, some of whom are holding objects, possibly tools or equipment. The presence of a van and a bus suggests a possible business or office setting. There is no indication of any suspicious activity or incident.\n",
      "Clip 038 | 00:01:16 - 00:01:18 | ALERT | motion=3.57 | 5.0s | ALERT: The scene appears to be a parking lot with several vehicles, including a blue pickup truck and a black car. There are three people visible in the frame, with one person wearing a yellow shirt and another wearing a blue hat. The third person is not clearly visible. The vehicles are parked in a row, and there is no indication of any suspicious activity or incident.\n",
      "Clip 039 | 00:01:18 - 00:01:20 | LOG   | motion=2.56 | 2.4s | ALERT: The scene appears to be a typical parking lot with several vehicles parked. There are no suspicious activities or incidents visible in the video.\n",
      "Clip 040 | 00:01:20 - 00:01:22 | LOG   | motion=2.03 | 2.4s | ALERT: The scene appears to be a typical parking lot with several vehicles parked. There are no suspicious activities or incidents visible in the video.\n",
      "Clip 041 | 00:01:22 - 00:01:24 | LOG   | motion=1.00 | 2.4s | ALERT: The scene appears to be a typical parking lot with several vehicles parked. There are no suspicious activities or incidents visible in the video.\n",
      "Clip 042 | 00:01:24 - 00:01:26 | LOG   | motion=1.34 | 2.4s | ALERT: The scene appears to be a typical parking lot with several vehicles parked. There are no suspicious activities or incidents visible in the video.\n",
      "Clip 043 | 00:01:26 - 00:01:27 | ALERT | motion=1.28 | 2.4s | ALERT: The scene appears to be a typical parking lot with several vehicles parked. There are no suspicious activities or incidents visible in the video.\n",
      "\n",
      "Done in 1.1 minutes.\n",
      "Processed 44 clips (21 skipped for low motion)\n",
      "Average: 2.8s per analyzed clip\n",
      "Readable log: outputs\\cctv_clips.txt\n",
      "JSONL log: outputs\\cctv_clips.jsonl\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import time\n",
    "from collections import deque\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "from transformers import Qwen2VLForConditionalGeneration, AutoProcessor\n",
    "from qwen_vl_utils import process_vision_info\n",
    "\n",
    "\n",
    "# =========================\n",
    "# USER CONFIG\n",
    "# =========================\n",
    "VIDEO_PATH = r\"A:\\Context Aware CCTV Surviellance system\\parking.mp4\"\n",
    "MODEL_ID = r\"A:\\hf_models\\Qwen2-VL-2B-Instruct\"\n",
    "\n",
    "# Video processing\n",
    "TARGET_FPS = 30.0                # Downsample high-FPS videos to this (speeds up reading)\n",
    "CLIP_DURATION_SEC = 2.0          # Analyze every 2-second clip\n",
    "FRAMES_PER_CLIP = 3              # Sample 3 representative frames from each clip\n",
    "MOTION_THRESH_CLIP = 1.0         # Skip clip if avg motion < threshold\n",
    "\n",
    "# Image resolution (balanced: good detail + reasonable speed)\n",
    "MIN_PIXELS = 256 * 28 * 28\n",
    "MAX_PIXELS = 1024 * 28 * 28\n",
    "\n",
    "# Generation\n",
    "MAX_NEW_TOKENS = 80\n",
    "\n",
    "# Alert smoothing\n",
    "WINDOW_N = 3\n",
    "TRIGGER_K = 2\n",
    "ALERT_COOLDOWN_SEC = 10\n",
    "\n",
    "# Output\n",
    "OUT_DIR = \"outputs\"\n",
    "LOG_TXT = os.path.join(OUT_DIR, \"cctv_clips.txt\")\n",
    "LOG_JSONL = os.path.join(OUT_DIR, \"cctv_clips.jsonl\")\n",
    "\n",
    "\n",
    "# =========================\n",
    "# HELPERS\n",
    "# =========================\n",
    "def ensure_dir(p):\n",
    "    os.makedirs(p, exist_ok=True)\n",
    "\n",
    "def fmt_hhmmss(seconds: float) -> str:\n",
    "    seconds = int(max(0, seconds))\n",
    "    h = seconds // 3600\n",
    "    m = (seconds % 3600) // 60\n",
    "    s = seconds % 60\n",
    "    return f\"{h:02d}:{m:02d}:{s:02d}\"\n",
    "\n",
    "def fmt_time_range(start_sec: float, end_sec: float) -> str:\n",
    "    return f\"{fmt_hhmmss(start_sec)} - {fmt_hhmmss(end_sec)}\"\n",
    "\n",
    "def write_line(path, line: str):\n",
    "    with open(path, \"a\", encoding=\"utf-8\") as f:\n",
    "        f.write(line.rstrip() + \"\\n\")\n",
    "\n",
    "def write_jsonl(path, obj: dict):\n",
    "    import json\n",
    "    with open(path, \"a\", encoding=\"utf-8\") as f:\n",
    "        f.write(json.dumps(obj, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "def motion_score(prev_bgr, curr_bgr) -> float:\n",
    "    prev = cv2.cvtColor(prev_bgr, cv2.COLOR_BGR2GRAY)\n",
    "    curr = cv2.cvtColor(curr_bgr, cv2.COLOR_BGR2GRAY)\n",
    "    prev = cv2.resize(prev, (160, 90))\n",
    "    curr = cv2.resize(curr, (160, 90))\n",
    "    return float(np.mean(cv2.absdiff(prev, curr)))\n",
    "\n",
    "\n",
    "# =========================\n",
    "# LOAD MODEL\n",
    "# =========================\n",
    "ensure_dir(OUT_DIR)\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    min_pixels=MIN_PIXELS,\n",
    "    max_pixels=MAX_PIXELS,\n",
    ")\n",
    "\n",
    "model = Qwen2VLForConditionalGeneration.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "model.eval()\n",
    "\n",
    "print(\"Model device:\", next(model.parameters()).device)\n",
    "print(f\"Config: {FRAMES_PER_CLIP} frames/clip, {MAX_NEW_TOKENS} tokens, resolution {MIN_PIXELS//784}-{MAX_PIXELS//784} visual tokens\\n\")\n",
    "\n",
    "\n",
    "def vlm_describe_clip(frames_pil: list, time_range: str):\n",
    "    \"\"\"\n",
    "    frames_pil: list of PIL images sampled from the clip\n",
    "    Returns: (model_tag, detailed_description, raw_line)\n",
    "    \"\"\"\n",
    "    content = [\n",
    "        {\"type\": \"text\", \"text\": f\"CCTV {time_range}. {len(frames_pil)} frames in order.\"}\n",
    "    ]\n",
    "    for pil_img in frames_pil:\n",
    "        content.append({\"type\": \"image\", \"image\": pil_img})\n",
    "    \n",
    "    content.append({\"type\": \"text\", \"text\":\n",
    "        \"In 2 sentences: what's happening? \"\n",
    "        \"If suspicious (robbery, assault, weapon, accident, destruction, fire, vandalism, forced entry), start with ALERT:. \"\n",
    "        \"Else start with LOG and describe the scene.\"\n",
    "    })\n",
    "\n",
    "    messages = [{\"role\": \"user\", \"content\": content}]\n",
    "\n",
    "    prompt = processor.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True,\n",
    "        add_vision_id=True,\n",
    "    )\n",
    "\n",
    "    images, videos = process_vision_info(messages)\n",
    "    inputs = processor(\n",
    "        text=[prompt],\n",
    "        images=images,\n",
    "        videos=videos,\n",
    "        padding=True,\n",
    "        return_tensors=\"pt\",\n",
    "    ).to(model.device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        with torch.amp.autocast('cuda', dtype=torch.float16):\n",
    "            output_ids = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=MAX_NEW_TOKENS,\n",
    "                do_sample=False,\n",
    "                num_beams=1,\n",
    "                use_cache=True,\n",
    "            )\n",
    "\n",
    "    # Decode only new tokens\n",
    "    trimmed = [out_ids[len(in_ids):] for in_ids, out_ids in zip(inputs.input_ids, output_ids)]\n",
    "    text = processor.batch_decode(\n",
    "        trimmed,\n",
    "        skip_special_tokens=True,\n",
    "        clean_up_tokenization_spaces=True,\n",
    "    )[0].strip()\n",
    "\n",
    "    # Parse tag\n",
    "    up = text.upper()\n",
    "    if up.startswith(\"ALERT:\"):\n",
    "        tag = \"ALERT\"\n",
    "        desc = text.split(\":\", 1)[1].strip() if \":\" in text else text\n",
    "    elif up.startswith(\"LOG:\"):\n",
    "        tag = \"LOG\"\n",
    "        desc = text.split(\":\", 1)[1].strip() if \":\" in text else text\n",
    "    else:\n",
    "        tag = \"LOG\"\n",
    "        desc = text\n",
    "\n",
    "    raw_line = f\"{tag}: {desc}\"\n",
    "    return tag, desc, raw_line\n",
    "\n",
    "\n",
    "# =========================\n",
    "# VIDEO LOOP (CLIP-BASED)\n",
    "# =========================\n",
    "cap = cv2.VideoCapture(VIDEO_PATH)\n",
    "if not cap.isOpened():\n",
    "    raise RuntimeError(f\"Could not open video: {VIDEO_PATH}\")\n",
    "\n",
    "# FPS downsampling for speed\n",
    "original_fps = cap.get(cv2.CAP_PROP_FPS) or 25.0\n",
    "fps = min(TARGET_FPS, original_fps)\n",
    "frame_skip = max(1, int(original_fps / fps))\n",
    "\n",
    "total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "clip_frames_count = int(fps * CLIP_DURATION_SEC)\n",
    "\n",
    "print(f\"Original FPS: {original_fps:.1f}, Target FPS: {fps:.1f}, Frame skip: {frame_skip}\")\n",
    "print(f\"Total frames: {total_frames}, Clip size: {clip_frames_count} frames ({CLIP_DURATION_SEC}s)\")\n",
    "print(f\"Estimated {total_frames // (clip_frames_count * frame_skip)} clips to process\\n\")\n",
    "\n",
    "# Alert smoothing\n",
    "decision_window = deque(maxlen=WINDOW_N)\n",
    "last_alert_time = 0\n",
    "\n",
    "clip_num = 0\n",
    "skipped_low_motion = 0\n",
    "start_time = time.time()\n",
    "\n",
    "# Frame reading state\n",
    "frame_idx = 0\n",
    "\n",
    "while True:\n",
    "    # Calculate clip boundaries in original video frames\n",
    "    clip_start_frame = clip_num * clip_frames_count * frame_skip\n",
    "    clip_end_frame = clip_start_frame + (clip_frames_count * frame_skip)\n",
    "    \n",
    "    if clip_start_frame >= total_frames:\n",
    "        break\n",
    "    \n",
    "    # Set video position to clip start\n",
    "    cap.set(cv2.CAP_PROP_POS_FRAMES, clip_start_frame)\n",
    "    \n",
    "    clip_start_sec = clip_start_frame / original_fps\n",
    "    clip_end_sec = min(clip_end_frame, total_frames) / original_fps\n",
    "    time_range = fmt_time_range(clip_start_sec, clip_end_sec)\n",
    "    \n",
    "    # Read frames in this clip (with frame skipping for FPS reduction)\n",
    "    clip_frames_bgr = []\n",
    "    motion_scores = []\n",
    "    prev_bgr = None\n",
    "    \n",
    "    local_frame_idx = 0\n",
    "    frames_read = 0\n",
    "    \n",
    "    while frames_read < clip_frames_count:\n",
    "        ok, frame_bgr = cap.read()\n",
    "        if not ok:\n",
    "            break\n",
    "        \n",
    "        # Skip frames to achieve target FPS\n",
    "        if local_frame_idx % frame_skip == 0:\n",
    "            clip_frames_bgr.append(frame_bgr)\n",
    "            \n",
    "            # Track motion\n",
    "            if prev_bgr is not None:\n",
    "                motion_scores.append(motion_score(prev_bgr, frame_bgr))\n",
    "            prev_bgr = frame_bgr\n",
    "            frames_read += 1\n",
    "        \n",
    "        local_frame_idx += 1\n",
    "    \n",
    "    if len(clip_frames_bgr) < FRAMES_PER_CLIP:\n",
    "        clip_num += 1\n",
    "        continue\n",
    "    \n",
    "    avg_motion = np.mean(motion_scores) if motion_scores else 0.0\n",
    "    \n",
    "    # Skip low-motion clips (static scene)\n",
    "    if avg_motion < MOTION_THRESH_CLIP:\n",
    "        line = f\"Clip {clip_num:03d} | {time_range} | SKIP  | motion={avg_motion:.2f} | (no significant activity)\"\n",
    "        print(line)\n",
    "        write_line(LOG_TXT, line)\n",
    "        write_jsonl(LOG_JSONL, {\n",
    "            \"clip_num\": clip_num,\n",
    "            \"time_range\": time_range,\n",
    "            \"start_sec\": round(clip_start_sec, 3),\n",
    "            \"end_sec\": round(clip_end_sec, 3),\n",
    "            \"system_tag\": \"SKIP\",\n",
    "            \"avg_motion\": round(avg_motion, 3),\n",
    "            \"note\": \"low_motion_skip\",\n",
    "        })\n",
    "        skipped_low_motion += 1\n",
    "        clip_num += 1\n",
    "        continue\n",
    "    \n",
    "    # Sample representative frames evenly from the clip\n",
    "    indices = np.linspace(0, len(clip_frames_bgr) - 1, FRAMES_PER_CLIP, dtype=int)\n",
    "    sampled_bgr = [clip_frames_bgr[i] for i in indices]\n",
    "    \n",
    "    # Convert to PIL\n",
    "    frames_pil = []\n",
    "    for bgr in sampled_bgr:\n",
    "        rgb = cv2.cvtColor(bgr, cv2.COLOR_BGR2RGB)\n",
    "        frames_pil.append(Image.fromarray(rgb))\n",
    "    \n",
    "    # Get VLM description for this clip\n",
    "    clip_inference_start = time.time()\n",
    "    model_tag, desc, raw_line = vlm_describe_clip(frames_pil, time_range)\n",
    "    clip_inference_time = time.time() - clip_inference_start\n",
    "    \n",
    "    # Smooth alerts\n",
    "    positive = (model_tag == \"ALERT\")\n",
    "    decision_window.append(1 if positive else 0)\n",
    "    \n",
    "    should_alert = (sum(decision_window) >= TRIGGER_K) and ((time.time() - last_alert_time) > ALERT_COOLDOWN_SEC)\n",
    "    system_tag = \"ALERT\" if should_alert else \"LOG\"\n",
    "    if should_alert:\n",
    "        last_alert_time = time.time()\n",
    "    \n",
    "    # Log output with timing\n",
    "    line = f\"Clip {clip_num:03d} | {time_range} | {system_tag:<5} | motion={avg_motion:.2f} | {clip_inference_time:.1f}s | {raw_line}\"\n",
    "    print(line)\n",
    "    write_line(LOG_TXT, line)\n",
    "    \n",
    "    write_jsonl(LOG_JSONL, {\n",
    "        \"clip_num\": clip_num,\n",
    "        \"time_range\": time_range,\n",
    "        \"start_sec\": round(clip_start_sec, 3),\n",
    "        \"end_sec\": round(clip_end_sec, 3),\n",
    "        \"system_tag\": system_tag,\n",
    "        \"model_tag\": model_tag,\n",
    "        \"avg_motion\": round(avg_motion, 3),\n",
    "        \"inference_time_sec\": round(clip_inference_time, 3),\n",
    "        \"description\": desc,\n",
    "        \"raw_line\": raw_line,\n",
    "        \"window_sum\": int(sum(decision_window)),\n",
    "    })\n",
    "    \n",
    "    clip_num += 1\n",
    "\n",
    "cap.release()\n",
    "elapsed = time.time() - start_time\n",
    "processed_clips = clip_num - skipped_low_motion\n",
    "\n",
    "print(f\"\\nDone in {elapsed/60:.1f} minutes.\")\n",
    "print(f\"Processed {clip_num} clips ({skipped_low_motion} skipped for low motion)\")\n",
    "if processed_clips > 0:\n",
    "    print(f\"Average: {elapsed/processed_clips:.1f}s per analyzed clip\")\n",
    "print(\"Readable log:\", LOG_TXT)\n",
    "print(\"JSONL log:\", LOG_JSONL)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
