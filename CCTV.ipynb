{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "47a12336",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc770e85384a46f8b781a19f7067500d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/3.99G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded: A:\\hf_models\\Qwen2-VL-2B-Instruct\\model-00001-of-00002.safetensors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8cf36a87902140829d411f951a747032",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/429M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded: A:\\hf_models\\Qwen2-VL-2B-Instruct\\model-00002-of-00002.safetensors\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import hf_hub_download\n",
    "\n",
    "repo_id = \"Qwen/Qwen2-VL-2B-Instruct\"\n",
    "local_dir = r\"A:\\hf_models\\Qwen2-VL-2B-Instruct\"\n",
    "\n",
    "for fname in [\n",
    "    \"model-00001-of-00002.safetensors\",\n",
    "    \"model-00002-of-00002.safetensors\",\n",
    "]:\n",
    "    path = hf_hub_download(\n",
    "        repo_id=repo_id,\n",
    "        filename=fname,\n",
    "        local_dir=local_dir,\n",
    "        force_download=True,\n",
    "    )\n",
    "    print(\"Downloaded:\", path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0cf4565d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The image processor of type `Qwen2VLImageProcessor` is now loaded as a fast processor by default, even if the model checkpoint was saved with a slow processor. This is a breaking change and may produce slightly different outputs. To continue using the slow processor, instantiate this class with `use_fast=False`. \n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoProcessor\n",
    "processor = AutoProcessor.from_pretrained(\n",
    "    r\"A:\\hf_models\\Qwen2-VL-2B-Instruct\",\n",
    "    min_pixels=256*28*28,\n",
    "    max_pixels=1024*28*28,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "77182117",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b36736777e0492496e287944797b45c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/729 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model device: cuda:0\n",
      "FPS: 60.00287306574532 sample_every_frames: 60\n",
      "00:00:00 | LOG   | warming_up | Starting up.\n",
      "00:00:00 | LOG   | warming_up | Starting up.\n",
      "00:00:01 | LOG   | motion=8.672 | LOG: A car has hit a pedestrian on the sidewalk.\n",
      "00:00:02 | LOG   | motion=9.055 | LOG: A car has crashed into another car on the road.\n",
      "Done.\n",
      "Readable log: outputs\\cctv_events.txt\n",
      "JSONL log: outputs\\cctv_events.jsonl\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import time\n",
    "from collections import deque\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "from transformers import Qwen2VLForConditionalGeneration, AutoProcessor\n",
    "from qwen_vl_utils import process_vision_info\n",
    "\n",
    "\n",
    "# =========================\n",
    "# USER CONFIG\n",
    "# =========================\n",
    "VIDEO_PATH = r\"A:\\Context Aware CCTV Surviellance system\\accident.mp4\"\n",
    "MODEL_ID = r\"A:\\hf_models\\Qwen2-VL-2B-Instruct\"\n",
    "\n",
    "# Video sampling\n",
    "SAMPLE_EVERY_SEC = 1.0          # sample ~1 frame per second\n",
    "MOTION_MEANABS_THRESH = 6.0     # lower -> more sensitive; higher -> fewer VLM calls\n",
    "HEARTBEAT_SEC = 10.0            # force a description at least every N seconds (even if low motion)\n",
    "\n",
    "# Alert smoothing (reduce one-off false alerts)\n",
    "WINDOW_N = 5\n",
    "TRIGGER_K = 2\n",
    "ALERT_COOLDOWN_SEC = 15\n",
    "\n",
    "# Output\n",
    "OUT_DIR = \"outputs\"\n",
    "LOG_TXT = os.path.join(OUT_DIR, \"cctv_events.txt\")   # simple readable log\n",
    "LOG_JSONL = os.path.join(OUT_DIR, \"cctv_events.jsonl\")  # optional structured log\n",
    "\n",
    "\n",
    "# =========================\n",
    "# HELPERS\n",
    "# =========================\n",
    "def ensure_dir(p):\n",
    "    os.makedirs(p, exist_ok=True)\n",
    "\n",
    "def fmt_hhmmss(seconds: float) -> str:\n",
    "    seconds = int(max(0, seconds))\n",
    "    h = seconds // 3600\n",
    "    m = (seconds % 3600) // 60\n",
    "    s = seconds % 60\n",
    "    return f\"{h:02d}:{m:02d}:{s:02d}\"\n",
    "\n",
    "def write_line(path, line: str):\n",
    "    with open(path, \"a\", encoding=\"utf-8\") as f:\n",
    "        f.write(line.rstrip() + \"\\n\")\n",
    "\n",
    "def write_jsonl(path, obj: dict):\n",
    "    import json\n",
    "    with open(path, \"a\", encoding=\"utf-8\") as f:\n",
    "        f.write(json.dumps(obj, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "def motion_score(prev_bgr, curr_bgr) -> float:\n",
    "    prev = cv2.cvtColor(prev_bgr, cv2.COLOR_BGR2GRAY)\n",
    "    curr = cv2.cvtColor(curr_bgr, cv2.COLOR_BGR2GRAY)\n",
    "    prev = cv2.resize(prev, (160, 90))\n",
    "    curr = cv2.resize(curr, (160, 90))\n",
    "    return float(np.mean(cv2.absdiff(prev, curr)))\n",
    "\n",
    "\n",
    "# =========================\n",
    "# LOAD MODEL\n",
    "# =========================\n",
    "ensure_dir(OUT_DIR)\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    min_pixels=256 * 28 * 28,\n",
    "    max_pixels=1024 * 28 * 28,\n",
    ")\n",
    "\n",
    "model = Qwen2VLForConditionalGeneration.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    torch_dtype=\"auto\",\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "model.eval()\n",
    "\n",
    "print(\"Model device:\", next(model.parameters()).device)\n",
    "\n",
    "\n",
    "def vlm_one_line(frames_pil, ts_text: str):\n",
    "    \"\"\"\n",
    "    Returns: (model_tag, description, raw_line)\n",
    "    model_tag: \"ALERT\" or \"LOG\"\n",
    "    \"\"\"\n",
    "    messages = [{\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"text\", \"text\": f\"You are monitoring a home CCTV camera. Timestamp {ts_text}. Frames are in time order.\"},\n",
    "            {\"type\": \"image\", \"image\": frames_pil[0]},\n",
    "            {\"type\": \"image\", \"image\": frames_pil[1]},\n",
    "            {\"type\": \"image\", \"image\": frames_pil[2]},\n",
    "            {\"type\": \"text\", \"text\":\n",
    "                \"Write exactly ONE LINE.\\n\"\n",
    "                \"If anything suspicious/unsafe/unethical is happening (robbery/theft, assault/fight, weapon visible, accident/crash, fire/smoke, vandalism/destruction, forced entry), output:\\n\"\n",
    "                \"ALERT: <short description>\\n\"\n",
    "                \"Otherwise output:\\n\"\n",
    "                \"LOG: <short description>\\n\"\n",
    "                \"Also mention useful non-alert info if visible (e.g., a stray dog/animal entered, delivery person at door).\\n\"\n",
    "                \"No JSON. No extra lines.\"\n",
    "            },\n",
    "        ],\n",
    "    }]\n",
    "\n",
    "    prompt = processor.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True,\n",
    "        add_vision_id=True,\n",
    "    )\n",
    "\n",
    "    images, videos = process_vision_info(messages)\n",
    "    inputs = processor(\n",
    "        text=[prompt],\n",
    "        images=images,\n",
    "        videos=videos,\n",
    "        padding=True,\n",
    "        return_tensors=\"pt\",\n",
    "    ).to(model.device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output_ids = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=80,\n",
    "            do_sample=False,\n",
    "            temperature=0.0,\n",
    "        )\n",
    "\n",
    "    # Decode ONLY the newly generated tokens (avoid prompt echo that breaks parsing/format).\n",
    "    # This is the key fix for your “output not valid JSON / messy output” issue.\n",
    "    trimmed = [out_ids[len(in_ids):] for in_ids, out_ids in zip(inputs.input_ids, output_ids)]\n",
    "    text = processor.batch_decode(\n",
    "        trimmed,\n",
    "        skip_special_tokens=True,\n",
    "        clean_up_tokenization_spaces=True,\n",
    "    )[0].strip()\n",
    "\n",
    "    # Force single line + enforce prefix\n",
    "    text = \" \".join(text.splitlines()).strip()\n",
    "    up = text.upper()\n",
    "    if up.startswith(\"ALERT:\"):\n",
    "        tag = \"ALERT\"\n",
    "        desc = text.split(\":\", 1)[1].strip()\n",
    "    elif up.startswith(\"LOG:\"):\n",
    "        tag = \"LOG\"\n",
    "        desc = text.split(\":\", 1)[1].strip()\n",
    "    else:\n",
    "        tag = \"LOG\"\n",
    "        desc = text\n",
    "\n",
    "    # Keep it short for logs\n",
    "    desc = desc[:200].strip()\n",
    "    raw_line = f\"{tag}: {desc}\"\n",
    "    return tag, desc, raw_line\n",
    "\n",
    "\n",
    "# =========================\n",
    "# VIDEO LOOP\n",
    "# =========================\n",
    "cap = cv2.VideoCapture(VIDEO_PATH)\n",
    "if not cap.isOpened():\n",
    "    raise RuntimeError(f\"Could not open video: {VIDEO_PATH}\")\n",
    "\n",
    "fps = cap.get(cv2.CAP_PROP_FPS) or 25.0\n",
    "sample_every_frames = max(1, int(fps * SAMPLE_EVERY_SEC))\n",
    "heartbeat_frames = max(1, int(fps * HEARTBEAT_SEC))\n",
    "\n",
    "print(\"FPS:\", fps, \"sample_every_frames:\", sample_every_frames)\n",
    "\n",
    "frame_idx = 0\n",
    "last_sample_bgr = None\n",
    "\n",
    "# 3 sampled frames: t-1, t, t+1\n",
    "sample_buf_bgr = deque(maxlen=3)\n",
    "\n",
    "# alert smoothing\n",
    "decision_window = deque(maxlen=WINDOW_N)\n",
    "last_alert_time = 0\n",
    "\n",
    "# for better “what is happening” logging\n",
    "last_desc = \"Starting up.\"\n",
    "last_model_tag = \"LOG\"\n",
    "last_vlm_frame_idx = -10**9\n",
    "\n",
    "while True:\n",
    "    ok, frame_bgr = cap.read()\n",
    "    if not ok:\n",
    "        break\n",
    "\n",
    "    if frame_idx % sample_every_frames != 0:\n",
    "        frame_idx += 1\n",
    "        continue\n",
    "\n",
    "    t_sec = frame_idx / fps\n",
    "    ts = fmt_hhmmss(t_sec)\n",
    "\n",
    "    # motion score on sampled frames\n",
    "    if last_sample_bgr is None:\n",
    "        score = 999.0\n",
    "    else:\n",
    "        score = motion_score(last_sample_bgr, frame_bgr)\n",
    "    last_sample_bgr = frame_bgr.copy()\n",
    "\n",
    "    sample_buf_bgr.append(frame_bgr)\n",
    "\n",
    "    # Need 3 frames before calling VLM\n",
    "    if len(sample_buf_bgr) < 3:\n",
    "        line = f\"{ts} | LOG   | warming_up | {last_desc}\"\n",
    "        print(line)\n",
    "        write_line(LOG_TXT, line)\n",
    "        write_jsonl(LOG_JSONL, {\"ts\": ts, \"t_sec\": round(t_sec, 3), \"tag\": \"LOG\", \"note\": \"warming_up\", \"desc\": last_desc})\n",
    "        frame_idx += 1\n",
    "        continue\n",
    "\n",
    "    # Decide whether to call VLM now\n",
    "    due_heartbeat = (frame_idx - last_vlm_frame_idx) >= heartbeat_frames\n",
    "    due_motion = score >= MOTION_MEANABS_THRESH\n",
    "    should_call_vlm = due_motion or due_heartbeat\n",
    "\n",
    "    if should_call_vlm:\n",
    "        frames_pil = []\n",
    "        for bgr in list(sample_buf_bgr):\n",
    "            rgb = cv2.cvtColor(bgr, cv2.COLOR_BGR2RGB)\n",
    "            frames_pil.append(Image.fromarray(rgb))\n",
    "\n",
    "        model_tag, desc, raw_line = vlm_one_line(frames_pil, ts_text=ts)\n",
    "        last_desc = desc\n",
    "        last_model_tag = model_tag\n",
    "        last_vlm_frame_idx = frame_idx\n",
    "\n",
    "        # Smooth alerts\n",
    "        positive = (model_tag == \"ALERT\")\n",
    "        decision_window.append(1 if positive else 0)\n",
    "\n",
    "        should_alert = (sum(decision_window) >= TRIGGER_K) and ((time.time() - last_alert_time) > ALERT_COOLDOWN_SEC)\n",
    "        system_tag = \"ALERT\" if should_alert else \"LOG\"\n",
    "        if should_alert:\n",
    "            last_alert_time = time.time()\n",
    "\n",
    "        line = f\"{ts} | {system_tag:<5} | motion={score:.3f} | {raw_line}\"\n",
    "        print(line)\n",
    "        write_line(LOG_TXT, line)\n",
    "        write_jsonl(LOG_JSONL, {\n",
    "            \"ts\": ts,\n",
    "            \"t_sec\": round(t_sec, 3),\n",
    "            \"system_tag\": system_tag,\n",
    "            \"model_tag\": model_tag,\n",
    "            \"motion_score\": round(score, 3),\n",
    "            \"description\": desc,\n",
    "            \"raw_line\": raw_line,\n",
    "            \"window_sum\": int(sum(decision_window)),\n",
    "        })\n",
    "    else:\n",
    "        # Even when skipping VLM, keep logs readable by repeating last known description\n",
    "        line = f\"{ts} | LOG   | motion={score:.3f} | last={last_model_tag}: {last_desc}\"\n",
    "        print(line)\n",
    "        write_line(LOG_TXT, line)\n",
    "        write_jsonl(LOG_JSONL, {\n",
    "            \"ts\": ts,\n",
    "            \"t_sec\": round(t_sec, 3),\n",
    "            \"system_tag\": \"LOG\",\n",
    "            \"note\": \"skipped_vlm_low_motion\",\n",
    "            \"motion_score\": round(score, 3),\n",
    "            \"last_model_tag\": last_model_tag,\n",
    "            \"last_description\": last_desc,\n",
    "        })\n",
    "\n",
    "    frame_idx += 1\n",
    "\n",
    "cap.release()\n",
    "print(\"Done.\")\n",
    "print(\"Readable log:\", LOG_TXT)\n",
    "print(\"JSONL log:\", LOG_JSONL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cec2ed26",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The image processor of type `Qwen2VLImageProcessor` is now loaded as a fast processor by default, even if the model checkpoint was saved with a slow processor. This is a breaking change and may produce slightly different outputs. To continue using the slow processor, instantiate this class with `use_fast=False`. \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1dc4415fb49f4f05baa0ffb3e3250742",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/729 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model device: cuda:0\n",
      "Speed optimizations: 4 frames/clip, 80 max tokens, resolution 128-512 visual tokens\n",
      "FPS: 25.0, Total frames: 906, Clip size: 100 frames (4.0s)\n",
      "Estimated 9 clips to process\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clip 000 | 00:00:00 - 00:00:04 | LOG   | motion=2.68 | 3.6s | ALERT: The CCTV footage shows two individuals entering a jewelry store. They are seen walking towards the display case and appear to be looking at the items.\n",
      "Clip 001 | 00:00:04 - 00:00:08 | ALERT | motion=1.82 | 5.9s | ALERT: The scene shows a long table in a jewelry store with several display cases. Two individuals are present, one of whom is holding a gun. The other individual is standing nearby. The setting appears to be a well-lit, modern store with a polished floor and a dark-colored wall. The individuals are dressed in dark clothing, and the gun is visible in their hands. The atmosphere suggests a\n",
      "Clip 002 | 00:00:08 - 00:00:12 | LOG   | motion=1.05 | 5.6s | LOG: A person wearing a black hoodie and black pants is walking towards a long table in a jewelry store. The table is covered with various jewelry displays. The person appears to be carrying a black bag. The camera angle is from the security camera's perspective. The time stamp on the screen indicates that this is a security camera recording from January 15, 2013, at\n",
      "Clip 003 | 00:00:12 - 00:00:16 | LOG   | motion=1.66 | 5.7s | LOG: CCTV footage shows a person in a black hoodie and black pants walking into a jewelry store. They are holding a large hammer and appear to be preparing to rob the store. The person is wearing a black hood and is walking towards the front of the store. The store has a long display table with jewelry on it. The person is wearing black pants and is walking towards the front of the store\n",
      "Clip 004 | 00:00:16 - 00:00:20 | LOG   | motion=0.69 | 5.8s | LOG: CCTV footage shows a person in a black hoodie and black pants walking towards a long table in a store. The person appears to be carrying a bag. The table is covered with jewelry displays. The footage is from a security camera and was recorded on January 15, 2013, at 18:13:25. The person is the only person visible\n",
      "Clip 005 | 00:00:20 - 00:00:24 | LOG   | motion=1.37 | 5.8s | LOG: CCTV footage shows a person in a black hoodie and black pants walking towards a long table in a jewelry store. The person is holding a hammer and appears to be in the process of breaking into the store. The footage is from a security camera and shows the person from various angles. The store has a long table with several display cases and vases on it. The person is wearing a black\n",
      "Clip 006 | 00:00:24 - 00:00:28 | LOG   | motion=0.58 | 4.9s | LOG: CCTV footage shows a person walking through a jewelry store with a weapon. The footage is from a security camera and was recorded on January 15, 2013, at 18:13:34. The person appears to be a suspect in a robbery or assault.\n",
      "Clip 007 | 00:00:28 - 00:00:32 | SKIP  | avg_motion=0.49 | (no significant activity)\n",
      "Clip 008 | 00:00:32 - 00:00:36 | LOG   | motion=0.59 | 1.9s | LOG: CCTV footage shows a person walking into a jewelry store.\n",
      "Clip 009 | 00:00:36 - 00:00:36 | LOG   | motion=0.65 | 3.0s | ALERT: The CCTV footage shows a person walking into a jewelry store and looking at the display. There are no signs of suspicious activity or incidents in the footage.\n",
      "\n",
      "Done in 0.7 minutes.\n",
      "Processed 10 clips (1 skipped for low motion)\n",
      "Average: 4.8s per analyzed clip\n",
      "Readable log: outputs\\cctv_clips.txt\n",
      "JSONL log: outputs\\cctv_clips.jsonl\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import time\n",
    "from collections import deque\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "from transformers import Qwen2VLForConditionalGeneration, AutoProcessor\n",
    "from qwen_vl_utils import process_vision_info\n",
    "\n",
    "\n",
    "# =========================\n",
    "# USER CONFIG\n",
    "# =========================\n",
    "VIDEO_PATH = r\"A:\\Context Aware CCTV Surviellance system\\input.mp4\"\n",
    "MODEL_ID = r\"A:\\hf_models\\Qwen2-VL-2B-Instruct\"\n",
    "\n",
    "# SPEED OPTIMIZATIONS:\n",
    "CLIP_DURATION_SEC = 4.0\n",
    "FRAMES_PER_CLIP = 4              # Reduced from 5 → 40% faster preprocessing\n",
    "MOTION_THRESH_CLIP = 0.5         # Skip boring clips (lower = more sensitive)\n",
    "\n",
    "# Image resolution (lower = faster, but less detail)\n",
    "MIN_PIXELS = 128 * 28 * 28       # Reduced from 256 → faster image encoding\n",
    "MAX_PIXELS = 512 * 28 * 28       # Reduced from 1024 → faster image encoding\n",
    "\n",
    "# Generation\n",
    "MAX_NEW_TOKENS = 80              # Reduced from 150 → faster generation\n",
    "\n",
    "# Alert smoothing\n",
    "WINDOW_N = 3\n",
    "TRIGGER_K = 2\n",
    "ALERT_COOLDOWN_SEC = 10\n",
    "\n",
    "# Output\n",
    "OUT_DIR = \"outputs\"\n",
    "LOG_TXT = os.path.join(OUT_DIR, \"cctv_clips.txt\")\n",
    "LOG_JSONL = os.path.join(OUT_DIR, \"cctv_clips.jsonl\")\n",
    "\n",
    "\n",
    "# =========================\n",
    "# HELPERS\n",
    "# =========================\n",
    "def ensure_dir(p):\n",
    "    os.makedirs(p, exist_ok=True)\n",
    "\n",
    "def fmt_hhmmss(seconds: float) -> str:\n",
    "    seconds = int(max(0, seconds))\n",
    "    h = seconds // 3600\n",
    "    m = (seconds % 3600) // 60\n",
    "    s = seconds % 60\n",
    "    return f\"{h:02d}:{m:02d}:{s:02d}\"\n",
    "\n",
    "def fmt_time_range(start_sec: float, end_sec: float) -> str:\n",
    "    return f\"{fmt_hhmmss(start_sec)} - {fmt_hhmmss(end_sec)}\"\n",
    "\n",
    "def write_line(path, line: str):\n",
    "    with open(path, \"a\", encoding=\"utf-8\") as f:\n",
    "        f.write(line.rstrip() + \"\\n\")\n",
    "\n",
    "def write_jsonl(path, obj: dict):\n",
    "    import json\n",
    "    with open(path, \"a\", encoding=\"utf-8\") as f:\n",
    "        f.write(json.dumps(obj, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "def motion_score(prev_bgr, curr_bgr) -> float:\n",
    "    prev = cv2.cvtColor(prev_bgr, cv2.COLOR_BGR2GRAY)\n",
    "    curr = cv2.cvtColor(curr_bgr, cv2.COLOR_BGR2GRAY)\n",
    "    prev = cv2.resize(prev, (160, 90))\n",
    "    curr = cv2.resize(curr, (160, 90))\n",
    "    return float(np.mean(cv2.absdiff(prev, curr)))\n",
    "\n",
    "\n",
    "# =========================\n",
    "# LOAD MODEL\n",
    "# =========================\n",
    "ensure_dir(OUT_DIR)\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    min_pixels=MIN_PIXELS,\n",
    "    max_pixels=MAX_PIXELS,\n",
    ")\n",
    "\n",
    "model = Qwen2VLForConditionalGeneration.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    torch_dtype=torch.float16,    # Use FP16 for faster inference\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "model.eval()\n",
    "\n",
    "print(\"Model device:\", next(model.parameters()).device)\n",
    "print(f\"Speed optimizations: {FRAMES_PER_CLIP} frames/clip, {MAX_NEW_TOKENS} max tokens, resolution {MIN_PIXELS//784}-{MAX_PIXELS//784} visual tokens\")\n",
    "\n",
    "\n",
    "def vlm_describe_clip(frames_pil: list, time_range: str):\n",
    "    \"\"\"\n",
    "    frames_pil: list of PIL images sampled from the clip\n",
    "    Returns: (model_tag, detailed_description, raw_line)\n",
    "    \"\"\"\n",
    "    # Shorter prompt = faster processing\n",
    "    content = [\n",
    "        {\"type\": \"text\", \"text\": f\"CCTV {time_range}. {len(frames_pil)} frames in order.\"}\n",
    "    ]\n",
    "    for pil_img in frames_pil:\n",
    "        content.append({\"type\": \"image\", \"image\": pil_img})\n",
    "    \n",
    "    content.append({\"type\": \"text\", \"text\":\n",
    "        \"In maximum 2 sentences: what's happening? \"\n",
    "        \"If suspicious (lethals, robbery, assault, weapon, accident, fire, vandalism, forced entry, running), start with ALERT:. \"\n",
    "        \"Otherwise start with LOG:\"\n",
    "    })\n",
    "\n",
    "    messages = [{\"role\": \"user\", \"content\": content}]\n",
    "\n",
    "    prompt = processor.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True,\n",
    "        add_vision_id=True,\n",
    "    )\n",
    "\n",
    "    images, videos = process_vision_info(messages)\n",
    "    inputs = processor(\n",
    "        text=[prompt],\n",
    "        images=images,\n",
    "        videos=videos,\n",
    "        padding=True,\n",
    "        return_tensors=\"pt\",\n",
    "    ).to(model.device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Mixed precision inference for speed\n",
    "        with torch.amp.autocast('cuda', dtype=torch.float16):\n",
    "            output_ids = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=MAX_NEW_TOKENS,\n",
    "                do_sample=False,\n",
    "                num_beams=1,\n",
    "                use_cache=True,\n",
    "            )\n",
    "\n",
    "\n",
    "    # Decode only new tokens (trim prompt)\n",
    "    trimmed = [out_ids[len(in_ids):] for in_ids, out_ids in zip(inputs.input_ids, output_ids)]\n",
    "    text = processor.batch_decode(\n",
    "        trimmed,\n",
    "        skip_special_tokens=True,\n",
    "        clean_up_tokenization_spaces=True,\n",
    "    )[0].strip()\n",
    "\n",
    "    # Parse tag\n",
    "    up = text.upper()\n",
    "    if up.startswith(\"ALERT:\"):\n",
    "        tag = \"ALERT\"\n",
    "        desc = text.split(\":\", 1)[1].strip() if \":\" in text else text\n",
    "    elif up.startswith(\"LOG:\"):\n",
    "        tag = \"LOG\"\n",
    "        desc = text.split(\":\", 1)[1].strip() if \":\" in text else text\n",
    "    else:\n",
    "        tag = \"LOG\"\n",
    "        desc = text\n",
    "\n",
    "    raw_line = f\"{tag}: {desc}\"\n",
    "    return tag, desc, raw_line\n",
    "\n",
    "\n",
    "# =========================\n",
    "# VIDEO LOOP (CLIP-BASED)\n",
    "# =========================\n",
    "cap = cv2.VideoCapture(VIDEO_PATH)\n",
    "if not cap.isOpened():\n",
    "    raise RuntimeError(f\"Could not open video: {VIDEO_PATH}\")\n",
    "\n",
    "fps = cap.get(cv2.CAP_PROP_FPS) or 25.0\n",
    "total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "clip_frames_count = int(fps * CLIP_DURATION_SEC)\n",
    "\n",
    "print(f\"FPS: {fps}, Total frames: {total_frames}, Clip size: {clip_frames_count} frames ({CLIP_DURATION_SEC}s)\")\n",
    "print(f\"Estimated {total_frames // clip_frames_count} clips to process\\n\")\n",
    "\n",
    "# Alert smoothing\n",
    "decision_window = deque(maxlen=WINDOW_N)\n",
    "last_alert_time = 0\n",
    "\n",
    "clip_num = 0\n",
    "skipped_low_motion = 0\n",
    "start_time = time.time()\n",
    "\n",
    "while True:\n",
    "    # Calculate clip boundaries\n",
    "    clip_start_frame = clip_num * clip_frames_count\n",
    "    clip_end_frame = clip_start_frame + clip_frames_count\n",
    "    \n",
    "    if clip_start_frame >= total_frames:\n",
    "        break\n",
    "    \n",
    "    # Set video position to clip start\n",
    "    cap.set(cv2.CAP_PROP_POS_FRAMES, clip_start_frame)\n",
    "    \n",
    "    clip_start_sec = clip_start_frame / fps\n",
    "    clip_end_sec = min(clip_end_frame, total_frames) / fps\n",
    "    time_range = fmt_time_range(clip_start_sec, clip_end_sec)\n",
    "    \n",
    "    # Read all frames in this clip\n",
    "    clip_frames_bgr = []\n",
    "    motion_scores = []\n",
    "    prev_bgr = None\n",
    "    \n",
    "    for local_idx in range(clip_frames_count):\n",
    "        ok, frame_bgr = cap.read()\n",
    "        if not ok:\n",
    "            break\n",
    "        clip_frames_bgr.append(frame_bgr)\n",
    "        \n",
    "        # Track motion\n",
    "        if prev_bgr is not None:\n",
    "            motion_scores.append(motion_score(prev_bgr, frame_bgr))\n",
    "        prev_bgr = frame_bgr\n",
    "    \n",
    "    if len(clip_frames_bgr) < FRAMES_PER_CLIP:\n",
    "        clip_num += 1\n",
    "        continue\n",
    "    \n",
    "    avg_motion = np.mean(motion_scores) if motion_scores else 0.0\n",
    "    \n",
    "    # SPEED GATE: skip low-motion clips (static scene)\n",
    "    if avg_motion < MOTION_THRESH_CLIP:\n",
    "        line = f\"Clip {clip_num:03d} | {time_range} | SKIP  | (no significant activity)\"\n",
    "        print(line)\n",
    "        write_line(LOG_TXT, line)\n",
    "        write_jsonl(LOG_JSONL, {\n",
    "            \"clip_num\": clip_num,\n",
    "            \"time_range\": time_range,\n",
    "            \"start_sec\": round(clip_start_sec, 3),\n",
    "            \"end_sec\": round(clip_end_sec, 3),\n",
    "            \"system_tag\": \"SKIP\",\n",
    "            \"avg_motion\": round(avg_motion, 3),\n",
    "            \"note\": \"low_motion_skip\",\n",
    "        })\n",
    "        skipped_low_motion += 1\n",
    "        clip_num += 1\n",
    "        continue\n",
    "    \n",
    "    # Sample representative frames evenly from the clip\n",
    "    indices = np.linspace(0, len(clip_frames_bgr) - 1, FRAMES_PER_CLIP, dtype=int)\n",
    "    sampled_bgr = [clip_frames_bgr[i] for i in indices]\n",
    "    \n",
    "    # Convert to PIL\n",
    "    frames_pil = []\n",
    "    for bgr in sampled_bgr:\n",
    "        rgb = cv2.cvtColor(bgr, cv2.COLOR_BGR2RGB)\n",
    "        frames_pil.append(Image.fromarray(rgb))\n",
    "    \n",
    "    # Get VLM description for this clip\n",
    "    clip_inference_start = time.time()\n",
    "    model_tag, desc, raw_line = vlm_describe_clip(frames_pil, time_range)\n",
    "    clip_inference_time = time.time() - clip_inference_start\n",
    "    \n",
    "    # Smooth alerts\n",
    "    positive = (model_tag == \"ALERT\")\n",
    "    decision_window.append(1 if positive else 0)\n",
    "    \n",
    "    should_alert = (sum(decision_window) >= TRIGGER_K) and ((time.time() - last_alert_time) > ALERT_COOLDOWN_SEC)\n",
    "    system_tag = \"ALERT\" if should_alert else \"LOG\"\n",
    "    if should_alert:\n",
    "        last_alert_time = time.time()\n",
    "    \n",
    "    # Log output with timing\n",
    "    line = f\"Clip {clip_num:03d} | {time_range} | {system_tag:<5} | {clip_inference_time:.1f}s | {raw_line}\"\n",
    "    print(line)\n",
    "    write_line(LOG_TXT, line)\n",
    "    \n",
    "    write_jsonl(LOG_JSONL, {\n",
    "        \"clip_num\": clip_num,\n",
    "        \"time_range\": time_range,\n",
    "        \"start_sec\": round(clip_start_sec, 3),\n",
    "        \"end_sec\": round(clip_end_sec, 3),\n",
    "        \"system_tag\": system_tag,\n",
    "        \"model_tag\": model_tag,\n",
    "        \"avg_motion\": round(avg_motion, 3),\n",
    "        \"inference_time_sec\": round(clip_inference_time, 3),\n",
    "        \"description\": desc,\n",
    "        \"raw_line\": raw_line,\n",
    "        \"window_sum\": int(sum(decision_window)),\n",
    "    })\n",
    "    \n",
    "    clip_num += 1\n",
    "\n",
    "cap.release()\n",
    "elapsed = time.time() - start_time\n",
    "print(f\"\\nDone in {elapsed/60:.1f} minutes.\")\n",
    "print(f\"Processed {clip_num} clips ({skipped_low_motion} skipped for low motion)\")\n",
    "print(f\"Average: {elapsed/max(1, clip_num-skipped_low_motion):.1f}s per analyzed clip\")\n",
    "print(\"Readable log:\", LOG_TXT)\n",
    "print(\"JSONL log:\", LOG_JSONL)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
