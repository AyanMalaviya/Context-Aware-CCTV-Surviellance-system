{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0cf4565d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The image processor of type `Qwen2VLImageProcessor` is now loaded as a fast processor by default, even if the model checkpoint was saved with a slow processor. This is a breaking change and may produce slightly different outputs. To continue using the slow processor, instantiate this class with `use_fast=False`. \n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoProcessor\n",
    "processor = AutoProcessor.from_pretrained(\n",
    "    r\"A:\\hf_models\\Qwen2-VL-2B-Instruct\",\n",
    "    min_pixels=256*28*28,\n",
    "    max_pixels=1024*28*28,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77182117",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a36456a712940e8abf16e12b2b9df38",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "preprocessor_config.json:   0%|          | 0.00/347 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61df10fbd26845a6b2c04f123d35ecb2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "chat_template.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ad62078b409428da92fd2e2d3af5c1a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "760c4070ae5e447f85c680d9f4f2c848",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83a8ea3ad0e4412ebfc9891e6a6fb117",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "381e76eabe1949e0ad0a64a6bb85c9e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f25e94f879b41e497a2940908819e92",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (incomplete total...): 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d7ecacb221246e199496f0425c961ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import time\n",
    "import json\n",
    "import re\n",
    "from collections import deque\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "from transformers import Qwen2VLForConditionalGeneration, AutoProcessor\n",
    "from qwen_vl_utils import process_vision_info\n",
    "\n",
    "\n",
    "# =========================\n",
    "# USER CONFIG\n",
    "# =========================\n",
    "VIDEO_PATH = r\"A:\\Context Aware CCTV Surviellance system\\input.mp4\"\n",
    "\n",
    "# You can switch to quantized variants if you choose to download them instead:\n",
    "# MODEL_ID = \"Qwen/Qwen2-VL-2B-Instruct-AWQ\"\n",
    "# MODEL_ID = \"Qwen/Qwen2-VL-2B-Instruct-GPTQ-Int4\"\n",
    "MODEL_ID = r\"A:\\hf_models\\Qwen2-VL-2B-Instruct\"\n",
    "\n",
    "\n",
    "# Sampling and gating\n",
    "SAMPLE_EVERY_SEC = 1.0          # sample ~1 frame per second (recommended start)\n",
    "MOTION_MEANABS_THRESH = 6.0     # lower -> more sensitive; higher -> fewer VLM calls\n",
    "\n",
    "# Alert smoothing\n",
    "WINDOW_N = 5                    # last N VLM decisions\n",
    "TRIGGER_K = 2                   # alert if >=K positives in window\n",
    "ALERT_COOLDOWN_SEC = 15         # avoid spamming\n",
    "\n",
    "# Output\n",
    "OUT_DIR = \"outputs\"\n",
    "LOG_JSONL = os.path.join(OUT_DIR, \"cctv_events.jsonl\")\n",
    "\n",
    "\n",
    "# =========================\n",
    "# HELPERS\n",
    "# =========================\n",
    "def ensure_dir(p):\n",
    "    os.makedirs(p, exist_ok=True)\n",
    "\n",
    "def fmt_hhmmss(seconds: float) -> str:\n",
    "    seconds = int(max(0, seconds))\n",
    "    h = seconds // 3600\n",
    "    m = (seconds % 3600) // 60\n",
    "    s = seconds % 60\n",
    "    return f\"{h:02d}:{m:02d}:{s:02d}\"\n",
    "\n",
    "def extract_json(text: str):\n",
    "    \"\"\"\n",
    "    Tries to extract a JSON object from model text, even if it adds extra words.\n",
    "    \"\"\"\n",
    "    if text is None:\n",
    "        return None\n",
    "    text = text.strip()\n",
    "\n",
    "    # common case: valid JSON already\n",
    "    try:\n",
    "        return json.loads(text)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    # try to find first {...} block\n",
    "    m = re.search(r\"\\{.*\\}\", text, flags=re.DOTALL)\n",
    "    if not m:\n",
    "        return None\n",
    "    try:\n",
    "        return json.loads(m.group(0))\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def write_jsonl(path, obj):\n",
    "    with open(path, \"a\", encoding=\"utf-8\") as f:\n",
    "        f.write(json.dumps(obj, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "def motion_score(prev_bgr, curr_bgr):\n",
    "    \"\"\"\n",
    "    Cheap gate: mean absolute difference on grayscale downscaled frames.\n",
    "    \"\"\"\n",
    "    prev = cv2.cvtColor(prev_bgr, cv2.COLOR_BGR2GRAY)\n",
    "    curr = cv2.cvtColor(curr_bgr, cv2.COLOR_BGR2GRAY)\n",
    "    prev = cv2.resize(prev, (160, 90))\n",
    "    curr = cv2.resize(curr, (160, 90))\n",
    "    return float(np.mean(cv2.absdiff(prev, curr)))\n",
    "\n",
    "\n",
    "# =========================\n",
    "# LOAD MODEL\n",
    "# =========================\n",
    "ensure_dir(OUT_DIR)\n",
    "\n",
    "# Reduce VRAM/time by limiting visual tokens during preprocessing. [web:244]\n",
    "# These numbers are a practical starting point; adjust if you need more detail.\n",
    "processor = AutoProcessor.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    min_pixels=256 * 28 * 28,\n",
    "    max_pixels=1024 * 28 * 28,\n",
    ")\n",
    "\n",
    "model = Qwen2VLForConditionalGeneration.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    torch_dtype=\"auto\",\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "model.eval()\n",
    "\n",
    "print(\"Model device:\", next(model.parameters()).device)\n",
    "\n",
    "\n",
    "def vlm_analyze(frames_pil, ts_text):\n",
    "    \"\"\"\n",
    "    frames_pil: list of 3 PIL images (t-1, t, t+1)\n",
    "    Returns parsed dict (or fallback).\n",
    "    \"\"\"\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"text\", \"text\": f\"You are a CCTV safety monitor. Timestamp {ts_text}. Frames are in time order.\"},\n",
    "                {\"type\": \"image\", \"image\": frames_pil[0]},\n",
    "                {\"type\": \"image\", \"image\": frames_pil[1]},\n",
    "                {\"type\": \"image\", \"image\": frames_pil[2]},\n",
    "                {\"type\": \"text\", \"text\":\n",
    "                    \"Task: describe what is happening across these 3 frames and decide if it is critical/suspicious.\\n\"\n",
    "                    \"Critical/suspicious examples: robbery/theft, assault/fight, weapon visible, accident/crash, fire/smoke, vandalism/destruction, forced entry/break-in.\\n\"\n",
    "                    \"If it's normal activity, mark it non-critical.\\n\"\n",
    "                    \"Return ONLY JSON exactly in this schema:\\n\"\n",
    "                    \"{\"\n",
    "                    \"\\\"critical\\\": true/false,\"\n",
    "                    \"\\\"type\\\": \\\"robbery_theft|assault_fight|weapon_visible|accident_crash|fire_smoke|vandalism_destruction|forced_entry|normal|unknown\\\",\"\n",
    "                    \"\\\"confidence\\\": 0.0-1.0,\"\n",
    "                    \"\\\"description\\\": \\\"short\\\"\"\n",
    "                    \"}\\n\"\n",
    "                    \"If unsure, set critical=false, type=\\\"unknown\\\", confidence<=0.4.\"\n",
    "                },\n",
    "            ],\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    prompt = processor.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True,\n",
    "        add_vision_id=True,\n",
    "    )\n",
    "\n",
    "    images, videos = process_vision_info(messages)\n",
    "    inputs = processor(\n",
    "        text=[prompt],\n",
    "        images=images,\n",
    "        videos=videos,\n",
    "        padding=True,\n",
    "        return_tensors=\"pt\",\n",
    "    ).to(model.device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        gen_ids = model.generate(**inputs, max_new_tokens=160)\n",
    "\n",
    "    txt = processor.batch_decode(gen_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n",
    "    j = extract_json(txt)\n",
    "\n",
    "    if isinstance(j, dict):\n",
    "        # normalize fields\n",
    "        j.setdefault(\"critical\", False)\n",
    "        j.setdefault(\"type\", \"unknown\")\n",
    "        j.setdefault(\"confidence\", 0.0)\n",
    "        j.setdefault(\"description\", \"\")\n",
    "        return j, txt\n",
    "\n",
    "    # fallback\n",
    "    return {\n",
    "        \"critical\": False,\n",
    "        \"type\": \"unknown\",\n",
    "        \"confidence\": 0.0,\n",
    "        \"description\": \"Model output was not valid JSON\",\n",
    "    }, txt\n",
    "\n",
    "\n",
    "# =========================\n",
    "# VIDEO LOOP\n",
    "# =========================\n",
    "cap = cv2.VideoCapture(VIDEO_PATH)\n",
    "if not cap.isOpened():\n",
    "    raise RuntimeError(f\"Could not open video: {VIDEO_PATH}\")\n",
    "\n",
    "fps = cap.get(cv2.CAP_PROP_FPS) or 25.0\n",
    "sample_every_frames = max(1, int(fps * SAMPLE_EVERY_SEC))\n",
    "\n",
    "print(\"FPS:\", fps, \"sample_every_frames:\", sample_every_frames)\n",
    "\n",
    "frame_idx = 0\n",
    "last_sample_bgr = None\n",
    "\n",
    "# 3-frame buffer for VLM (t-1, t, t+1) on sampled frames\n",
    "sample_buf_bgr = deque(maxlen=3)\n",
    "\n",
    "# for smoothing alerts\n",
    "decision_window = deque(maxlen=WINDOW_N)\n",
    "last_alert_time = 0\n",
    "\n",
    "while True:\n",
    "    ok, frame_bgr = cap.read()\n",
    "    if not ok:\n",
    "        break\n",
    "\n",
    "    if frame_idx % sample_every_frames != 0:\n",
    "        frame_idx += 1\n",
    "        continue\n",
    "\n",
    "    t_sec = frame_idx / fps\n",
    "    ts = fmt_hhmmss(t_sec)\n",
    "\n",
    "    # motion gate\n",
    "    if last_sample_bgr is None:\n",
    "        score = 999.0\n",
    "    else:\n",
    "        score = motion_score(last_sample_bgr, frame_bgr)\n",
    "\n",
    "    last_sample_bgr = frame_bgr.copy()\n",
    "\n",
    "    # keep buffer of sampled frames\n",
    "    sample_buf_bgr.append(frame_bgr)\n",
    "\n",
    "    # If not enough temporal context, just log lightweight info\n",
    "    if len(sample_buf_bgr) < 3:\n",
    "        evt = {\n",
    "            \"ts\": ts,\n",
    "            \"t_sec\": round(t_sec, 3),\n",
    "            \"tag\": \"LOG\",\n",
    "            \"motion_score\": round(score, 3),\n",
    "            \"note\": \"warming_up_buffer\",\n",
    "        }\n",
    "        print(f\"{ts} | LOG   | motion={evt['motion_score']} | warming_up_buffer\")\n",
    "        write_jsonl(LOG_JSONL, evt)\n",
    "        frame_idx += 1\n",
    "        continue\n",
    "\n",
    "    # only call VLM if motion suggests \"something changed\"\n",
    "    if score < MOTION_MEANABS_THRESH:\n",
    "        evt = {\n",
    "            \"ts\": ts,\n",
    "            \"t_sec\": round(t_sec, 3),\n",
    "            \"tag\": \"LOG\",\n",
    "            \"motion_score\": round(score, 3),\n",
    "            \"note\": \"no_significant_change\",\n",
    "        }\n",
    "        print(f\"{ts} | LOG   | motion={evt['motion_score']} | no_significant_change\")\n",
    "        write_jsonl(LOG_JSONL, evt)\n",
    "        frame_idx += 1\n",
    "        continue\n",
    "\n",
    "    # Prepare 3 frames for the VLM\n",
    "    frames_pil = []\n",
    "    for bgr in list(sample_buf_bgr):\n",
    "        rgb = cv2.cvtColor(bgr, cv2.COLOR_BGR2RGB)\n",
    "        frames_pil.append(Image.fromarray(rgb))\n",
    "\n",
    "    result, raw_text = vlm_analyze(frames_pil, ts_text=ts)\n",
    "\n",
    "    critical = bool(result.get(\"critical\", False))\n",
    "    conf = float(result.get(\"confidence\", 0.0))\n",
    "    typ = str(result.get(\"type\", \"unknown\"))\n",
    "    desc = str(result.get(\"description\", \"\"))\n",
    "\n",
    "    # Decide alert vs log using smoothing\n",
    "    positive = critical and conf >= 0.70\n",
    "    decision_window.append(1 if positive else 0)\n",
    "\n",
    "    should_alert = (sum(decision_window) >= TRIGGER_K) and ((time.time() - last_alert_time) > ALERT_COOLDOWN_SEC)\n",
    "\n",
    "    tag = \"ALERT\" if should_alert else \"LOG\"\n",
    "    if should_alert:\n",
    "        last_alert_time = time.time()\n",
    "\n",
    "    # \"unethical\" tag requirement: mark unethical when critical\n",
    "    unethical = bool(critical)\n",
    "\n",
    "    evt = {\n",
    "        \"ts\": ts,\n",
    "        \"t_sec\": round(t_sec, 3),\n",
    "        \"tag\": tag,\n",
    "        \"unethical\": unethical,\n",
    "        \"motion_score\": round(score, 3),\n",
    "        \"critical\": critical,\n",
    "        \"type\": typ,\n",
    "        \"confidence\": round(conf, 3),\n",
    "        \"description\": desc[:240],\n",
    "        \"model_raw_preview\": raw_text[:240],\n",
    "        \"window_sum\": int(sum(decision_window)),\n",
    "    }\n",
    "\n",
    "    print(f\"{ts} | {tag:<5} | unethical={unethical} critical={critical} conf={evt['confidence']} type={typ} | {desc[:120]}\")\n",
    "    write_jsonl(LOG_JSONL, evt)\n",
    "\n",
    "    frame_idx += 1\n",
    "\n",
    "cap.release()\n",
    "print(\"Done. Logs:\", LOG_JSONL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cec2ed26",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to llava-hf/llava-onevision-qwen2-0.5b-ov-hf and revision 2c9ba3b.\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4801156f1a704bc58883ef9aeedd8f2f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b01f5ee789d4886b4551cf91b9aca65",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.79G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "from transformers import pipeline, BitsAndBytesConfig\n",
    "\n",
    "quant = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    ")\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"image-text-to-text\",\n",
    "    model=MODEL_ID,\n",
    "    model_kwargs={\"quantization_config\": quant, \"device_map\": \"auto\"},\n",
    ")\n",
    "\n",
    "# Confirm device (should say cuda:0 if GPU)\n",
    "print(next(pipe.model.parameters()).device)\n",
    "\n",
    "img = Image.open(\"test.png\").convert(\"RGB\")\n",
    "\n",
    "messages = [{\n",
    "    \"role\": \"user\",\n",
    "    \"content\": [\n",
    "        {\"type\": \"image\", \"image\": img},\n",
    "        {\"type\": \"text\", \"text\":\n",
    "            \"Traffic CCTV task: decide if there is a road accident/collision/crash in this frame. \"\n",
    "            \"Reply with exactly one sentence.\"\n",
    "        },\n",
    "    ],\n",
    "}]\n",
    "\n",
    "out = pipe(text=messages, max_new_tokens=5, max_length=None, return_full_text=False)\n",
    "print(out[0][\"generated_text\"])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
